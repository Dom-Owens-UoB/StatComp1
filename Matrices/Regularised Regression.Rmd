---
title: "Matrix Methods for Regularised Regression"
author: "Dom Owens"
date: "12/11/2019"
output:
  pdf_document: default
  html_document: default
---

In *regularised regression*, we solve the problem 
$$ \min_{\mathbf{w}} || \mathbf{y - w^T X}  || + \lambda R(\mathbf{w}) $$
where $R(\mathbf{w})$ is the regularisation function. Common choices include the l1- and l2-norms $R(\mathbf{w}) = || \mathbf{w}||_1$  and $R(\mathbf{w}) = || \mathbf{w}||^2$, which induce the LASSO and Ridge regressions respectively. 
Both of these regression methods perform well for high-dimensional data, and can be used to demonstrate associated matrix methods.



We consider a sparse dataset on blog posts.
```{r}
data_raw <- read.csv("blogData_train.csv", header = FALSE)  #read in data
index <- sample(1:dim(data_raw)[1], 5000)
data <- data_raw[index,]#construct sample
#head(data[1:2,])

X <- data[,-281] #create design matrix
y <-  data[,281] # assign response: hits/day
```



We compare the memory useage of storing `X` as a dense or sparse matrix.
```{r}
library(Matrix)
X_d <- matrix(unlist(X), nrow = dim(X)[1], ncol = dim(X)[2]) #as dense matrix
X_s <- Matrix(unlist(X), nrow = dim(X)[1], ncol = dim(X)[2], sparse = TRUE) #as sparse matrix
d <- as.numeric(object.size(X_d))
s <- as.numeric(object.size(X_s))
library(ggplot2)
ggplot(data = data.frame(c("X_d","X_s"), c(d,s)), aes(x= c("Dense","Sparse"), y = c(d,s), fill="red") ) + geom_bar(stat = "identity") + xlab("Matrix") + ylab("Memory use") + ggtitle("Memory Use by Matrix Storage Format") + theme_minimal() + guides(fill = "none") #visualise
```

How sparse is `X`?
```{r}
rankMatrix(X_s) # show rank
dim(X_s)[2] - rankMatrix(X_s)[1] # find nullity
```



## Ridge regression with sparse matrices
For the ridge regression $$ min_{\mathbf{w}} || \mathbf{y - w^T X}  || + \lambda ||\mathbf{w}||^2 $$

We have the closed-form solution 
$$ \mathbf{\hat{w}}^T = (XX^T + \lambda I)^{-1}X \mathbf{y}^T  $$
We compare performance for computing this estimate with dense and sparse matrices, respectively

```{r}
X_d1 <- t(cbind(1, X_d)) # attach column of intercept, then transpose
X_s1 <- t(cbind(1, X_s))
y_1 <- t(y)

ridge <- function(X, y, lambda){ #specify ridge regression 
  w <- solve(X %*% t(X) + diag(lambda, dim(X)[1])) %*% X %*% t(y)
  w
}


dense.times <- system.time(w_d <- ridge(X_d1, y_1, lambda = 5))
sparse.times <- system.time(w_s <- ridge(X_s1, y_1, lambda = 5))

```
```{r, echo=FALSE}
performance <- data.frame()
performance <- rbind(performance, data.frame(Format = 'Dense', #append dense performance
                                               UserTime = dense.times[1],
                                               SystemTime = dense.times[2],
                                               ElapsedTime = dense.times[3]))
performance <- rbind(performance, data.frame(Format = 'Sparse', #append sparse performance
                                               UserTime = sparse.times[1],
                                               SystemTime = sparse.times[2],
                                               ElapsedTime = sparse.times[3]))
```
                                                                                              

```{r, echo = FALSE}
par(mfrow = c(1,3))
ggplot(performance, aes(x = Format, y = UserTime, fill = Format)) +
  geom_bar(stat = "identity") +
  ylab('User Time in Seconds') 
 
ggplot(performance, aes(x = Format, y = SystemTime, fill = Format)) +
  geom_bar(stat = "identity") +
  ylab('System Time in Seconds') 
 
ggplot(performance, aes(x = Format, y = ElapsedTime, fill = Format)) +
  geom_bar(stat = "identity") +
  ylab('System Time in Seconds')
```

Interestingly, the dense calculations outperform the sparse calculations for this dataset. This would likely change for a more sparse set of data.

We see these are stored differently
```{r}
object.size(w_d)
head(w_d)
object.size(w_s)
head(w_s)
```
And differ by a median magnitude of 2e-11; most of this error comes from the leftmost non-sparse part of the matrix.
```{r}
error <- as.matrix(abs(w_d-w_s))
summary(error)
plot(error, main = "Coefficient Calculation Error")
```




## IRLS Under Sparsity
*Credit to https://bwlewis.github.io/GLM/ for non-ridge algorithm code; https://arxiv.org/pdf/1509.09169.pdf for algorithm*

A better approach to this might be to split the design matrix into dense and sparse partitions, and conduct separate calculations on each of these.
We use the **Iteratively Reweighted Least Squares** optimisation (procedure){https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares} 
for the $L_2$ norm regularisation.

We iteratively solve

$$ \boldsymbol{\beta} = \arg\min_{\beta} [X^TWX + \lambda I]^{-1}X^TWZ  $$

where $$ W_{ii} = \frac{1}{(y_i - X_i\beta^{(t)})} $$ are the diagonal entries of the weight matrix, which we update with each stage. 
$$Z = X \beta + W^{-1}(y - X\beta)$$ is the adjusted response.  

```{r, echo=FALSE}
#my_irls <- function(X, y, lambda = 0.5, family = gaussian,  maxit=25, tol=1e-08)
#{
#  beta <- rep(0,ncol(X))
#  for(j in 1:maxit)
#  {
#    eta    <- X %*% beta
#    g      <- family()$linkinv(eta)
#    gprime <- family()$mu.eta(eta)
#    z      <- eta + (y - g) / gprime
#    W      <- as.vector(gprime^2 / family()$variance(g))
#    beta_old   <- beta
#    beta      <- solve(crossprod(X,W*X), crossprod(X,W*z), tol=2*.Machine$double.eps)
#    if(sqrt(crossprod(x-xold)) < tol) break
#  }
#  list(coefficients=beta,iterations=j)
#}


```

```{r}
# Sparse weighted cross product helper function
# Input: Dense Matrix A_dense, sparse Matrix A_sparse, weights W,
# where A = [A_dense, A_sparse] and length W=ncol(A).
# Output: Dense representation of crossprod(A,W*A) + lambda*I
sp_wt_cross <- function(A_dense, A_sparse, W, lambda)
{
  nd <- ncol(A_dense)
  ns <- ncol(A_sparse)
  n  <- nd + ns
  ATWA <- matrix(0, nrow=n, ncol=n)
  WA_dense  <- W*A_dense
  WA_sparse <- W*A_sparse
  ATWA[1:nd,1:nd] <- as.matrix(crossprod(A_dense,WA_dense))
  ATWA[1:nd,(nd+1):n] <- as.matrix(crossprod(A_dense,WA_sparse))
  ATWA[(nd+1):n,1:nd] <- as.matrix(crossprod(A_sparse,WA_dense))
  ATWA[(nd+1):n,(nd+1):n] <- as.matrix(crossprod(A_sparse,WA_sparse))
  ATWA + diag(lambda, n) #add ridge term
}

# Example IRLS implementation that can take advantage of sparse model matrices.
# Here we assume that the model matrix A is already permuted and partitioned
# into A = [A_dense, A_sparse] dense and sparse columns.  The response vector b
# is also assumed to be permuted conformably with the matrix partitioning.

irls_sparse <-
function(A_dense, A_sparse, b, lambda =1, family=gaussian, maxit=25, tol=1e-08)
{
  nd <- ncol(A_dense)
  ns <- ncol(A_sparse)
  n  <- nd + ns
  x <- rep(0, n)
  for(j in 1:maxit)
  {
    eta   <- as.vector(A_dense %*% x[1:nd] + A_sparse %*% x[(nd+1):n])
    g     <- family()$linkinv(eta)
    gprime <- family()$mu.eta(eta)
    z     <- eta + (b - g) / gprime
    W     <- as.vector(gprime^2 / family()$variance(g))
    xold  <- x
    ATWA  <- sp_wt_cross(A_dense,A_sparse,W,lambda)
    wz    <- W*z
    ATWz  <- c(as.vector(crossprod(A_dense, wz)), as.vector(crossprod(A_sparse, wz)))

    C   <- chol(ATWA, pivot=TRUE)
   # if(attr(C,"rank") < ncol(ATWA)) stop("Rank-deficiency detected.")
    p   <- attr(C, "pivot")
    s   <- forwardsolve(t(C), ATWz[p])
    x   <- backsolve(C,s)[p]

    if(sqrt(crossprod(x-xold)) < tol) break
  }
  list(coefficients=x,iterations=j)
}
```


```{r}
X_dense <- as.matrix(X[,1:50])
X_sparse <- as.matrix(X[, 51:280])

irls.times <- system.time(w_irls <- irls_sparse(X_dense, X_sparse, y))
```

```{r}
performance <- rbind(performance, data.frame(Format = 'IRLS', #append dense performance
                                               UserTime = irls.times[1],
                                               SystemTime = irls.times[2],
                                               ElapsedTime = irls.times[3]))

par(mfrow = c(1,3))
ggplot(performance, aes(x = Format, y = UserTime, fill = Format)) +
  geom_bar(stat = "identity") +
  ylab('User Time in Seconds') 
 
ggplot(performance, aes(x = Format, y = SystemTime, fill = Format)) +
  geom_bar(stat = "identity") +
  ylab('System Time in Seconds') 
 
ggplot(performance, aes(x = Format, y = ElapsedTime, fill = Format)) +
  geom_bar(stat = "identity") +
  ylab('System Time in Seconds')
```

We can see that the IRLS hybrid method outperforms the sparse method, but the dense method performs best overall. 